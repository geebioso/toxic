{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import nltk \n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import ujson\n",
    "import sqlite3\n",
    "import sys\n",
    "import psycopg2\n",
    "import shutil\n",
    "import os\n",
    "import logging \n",
    "import csv \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "\n",
    "from scipy.sparse import hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONS \n",
    "REFRESH_DATABASE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with corpora information \n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\"host=localhost dbname=toxic_ml_features user=postgres\")\n",
    "except:\n",
    "    print(\"I am unable to connect to the database\")\n",
    "    \n",
    "# create cursor \n",
    "c = conn.cursor()\n",
    "\n",
    "if REFRESH_DATABASE:\n",
    "    print(\"Dropping classifiers table and removing Results/preds\")\n",
    "    c.execute('DROP TABLE corpora')\n",
    "    c.execute('DROP TABLE document_stats')\n",
    "#         try: \n",
    "#             shutil.rmtree('./Results/preds') \n",
    "#         except:\n",
    "#             print(\"directory ./Results/preds does not exist\")\n",
    "\n",
    "#     os.makedirs('./Results/preds')    \n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS corpora (\n",
    "             corpus_id text,\n",
    "             train_corpus_file text, \n",
    "             test_corpus_file json, \n",
    "             train_vectorizer text, \n",
    "             test_vectorizer text\n",
    "             )''')\n",
    "\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS document_stats (\n",
    "             feature_names json,\n",
    "             processing_options json\n",
    "         )''')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FEATURE GENERATION \n",
    "\n",
    "OVERLAP_MODE = 'remove'\n",
    "DO_REPEAT_SCORE = 1\n",
    "DOTEST = 0\n",
    "DO_NUM_NONZERO = 0\n",
    "DO_NUM_REPEATS = 0 \n",
    "DO_TOKENIZE_AND_STEM=1\n",
    "DO_CURSEWORD_COUNT = 1\n",
    "\n",
    "def tokenize_and_stem(comment, stemmer = SnowballStemmer(\"english\")):\n",
    "    \"\"\"\n",
    "    Takes a reddit comment and stemmer as input\n",
    "    and returns a list of stemmed words\n",
    "    \n",
    "    params: \n",
    "        str comment: comment to stem \n",
    "        nltk.stemmer stemmer: nltk stemmer object \n",
    "        \n",
    "    rtype: ls[str]\n",
    "    \"\"\"\n",
    "    comment = comment.strip().split()\n",
    "    stemmed_comment = [stemmer.stem(word) for word in comment]\n",
    "    return stemmed_comment\n",
    "\n",
    "# def generate_features( OVERLAP_MODE = 'remove', DO_REPEAT_SCORE = 1, \n",
    "#     DO_NUM_NONZERO = 0, DO_NUM_REPEATS = 0, DOTEST = 0):\n",
    "#     \"\"\"\n",
    "#     bool DOTEST: should we run in test mode?\n",
    "#     \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "    Options\n",
      "    OVERLAP_MODE: remove\n",
      "    DO_REPEAT_SCORE: 1\n",
      "    DOTEST: 0\n",
      "\n",
      "    ANALYZE: word\n",
      "    STRIP_ACCENTS: unicode\n",
      "    TOKENIZER: None\n",
      "    NGRAM_RANGE: (0, 2)\n",
      "    MAX_DF: 0.8\n",
      "    MIN_DF: 50\n",
      "    \n",
      "INFO:root:tokenizing comments\n",
      "INFO:root:Removing overlapping vocabulary in test and train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CORPUS GENERATION \n",
    "\n",
    "#### Options ##############################################################\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# parameters for vectorizer\n",
    "ANALYZER = \"word\" # unit of features are single words rather then phrases of words\n",
    "STRIP_ACCENTS = 'unicode'\n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) # Range for n-grams\n",
    "MAX_DF = 0.8  # Exclude words that are contained in more than x percent of documents\n",
    "MIN_DF = 50   # Exclude words that appear in less than 50 documents \n",
    "\n",
    "###########################################################################\n",
    "# read in data\n",
    "test_file = 'data/test.csv'\n",
    "train_file = 'data/train.csv'\n",
    "\n",
    "tr = pd.read_csv(train_file)\n",
    "te = pd.read_csv(test_file)\n",
    "\n",
    "if DOTEST:\n",
    "    tr = tr.iloc[1:1000]\n",
    "    te = te.iloc[1:1000]\n",
    "    MIN_DF = 10\n",
    "    MAX_DF = 1.0\n",
    "    \n",
    "Ntr = tr.shape[0]\n",
    "Nte = te.shape[0]\n",
    "\n",
    "logging.info( \"\"\"\n",
    "    Options\n",
    "    OVERLAP_MODE: {}\n",
    "    DO_REPEAT_SCORE: {}\n",
    "    DOTEST: {}\n",
    "\n",
    "    ANALYZE: {}\n",
    "    STRIP_ACCENTS: {}\n",
    "    TOKENIZER: {}\n",
    "    NGRAM_RANGE: {}\n",
    "    MAX_DF: {}\n",
    "    MIN_DF: {}\n",
    "    \"\"\".format( OVERLAP_MODE, DO_REPEAT_SCORE, DOTEST, ANALYZER, \\\n",
    "        STRIP_ACCENTS, TOKENIZER, NGRAM_RANGE, MAX_DF, MIN_DF) )\n",
    "\n",
    "train_ids = tr.id\n",
    "test_ids = te.id\n",
    "\n",
    "def preprocess_comment( comment, RE_PREPROCESS = \\\n",
    "    re.compile(r\"\"\" \\W + # one or more nonword characters\n",
    "        |    # the or operator\n",
    "        \\d+  # digits\"\"\", re.VERBOSE), \n",
    "    URL_PREPROCESS = re.compile(r\"\"\"[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\"\", \\\n",
    "        re.VERBOSE)):\n",
    "    \"\"\"\n",
    "    str comment:\n",
    "    compiled regular expression: RE_PREPROCESS \n",
    "    \"\"\"\n",
    "    return re.sub( RE_PREPROCESS, ' ', comment).lower()\n",
    "\n",
    "if DO_TOKENIZE_AND_STEM:\n",
    "    TOKENIZER = tokenize_and_stem\n",
    "else: \n",
    "    TOKENIZER = None \n",
    "    \n",
    "tr_vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer = TOKENIZER, \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            preprocessor = preprocess_comment, \n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "te_vectorizer = CountVectorizer(analyzer=ANALYZER,\n",
    "                            tokenizer = TOKENIZER, \n",
    "                            ngram_range=NGRAM_RANGE,\n",
    "                            preprocessor = preprocess_comment,\n",
    "                            stop_words = stopwords.words('english'),\n",
    "                            strip_accents=STRIP_ACCENTS,\n",
    "                            min_df = MIN_DF,\n",
    "                            max_df = MAX_DF)\n",
    "\n",
    "# tokenize\n",
    "logging.info(\"tokenizing comments\")\n",
    "train_bag_of_words = tr_vectorizer.fit_transform( tr['comment_text'])\n",
    "test_bag_of_words = te_vectorizer.fit_transform( te['comment_text'])\n",
    "\n",
    "tr_vocab = tr_vectorizer.get_feature_names()\n",
    "te_vocab = te_vectorizer.get_feature_names()\n",
    "\n",
    "if OVERLAP_MODE:\n",
    "    logging.info(\"Removing overlapping vocabulary in test and train\")\n",
    "    # compute percent overlap in vocabulary\n",
    "    overlap = set(tr_vocab).intersection( te_vocab )\n",
    "\n",
    "    # Remove any features that aren't in both training and test\n",
    "    train_in_overlap  = [i for i, word in enumerate(tr_vocab) if word in overlap]\n",
    "    test_in_overlap  = [i for i, word in enumerate(te_vocab) if word in overlap]\n",
    "    tr_vocab = [tr_vocab[i] for i in train_in_overlap]\n",
    "    te_vocab = [te_vocab[i] for i in test_in_overlap]\n",
    "    train_bag_of_words = train_bag_of_words[:,train_in_overlap]\n",
    "    test_bag_of_words = test_bag_of_words[:, test_in_overlap]\n",
    "\n",
    "\n",
    "#return train_bag_of_words, test_bag_of_words, train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing repeat score\n"
     ]
    }
   ],
   "source": [
    "# PRE VECTORIZED FEATURE GENERATION \n",
    "\n",
    "# regexp preprocessing\n",
    "RE_PREPROCESS = re.compile(r\"\"\" \\W + # one or more nonword characters\n",
    "                               |    # the or operator\n",
    "                               \\d+  # digits\"\"\", re.VERBOSE)\n",
    "train_comments = list( tr.comment_text )\n",
    "test_comments = list( te.comment_text )\n",
    "processed_tr_comments = [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in train_comments]\n",
    "processed_te_comments = [ re.sub(RE_PREPROCESS, ' ', comment).lower() for comment in test_comments] \n",
    "\n",
    "train_feat = pd.DataFrame( index = tr.index )\n",
    "test_feat = pd.DataFrame( index = te.index )\n",
    "\n",
    "# Generate repeat score\n",
    "if DO_REPEAT_SCORE:\n",
    "    logging.info(\"Computing repeat score\")\n",
    "    \n",
    "    tr_num_nonzero, tr_repeat_score, tr_num_repeats = [], [], []\n",
    "    for comment in processed_tr_comments:\n",
    "        vec = comment.strip().split() \n",
    "        tr_num_nonzero.append( len(set(vec)))\n",
    "        tr_num_repeats.append( len(vec))\n",
    "        tr_rep_score = ( len( set( vec)) + 1)/(len(vec) + 1) - 1\n",
    "        tr_repeat_score.append( tr_rep_score) \n",
    "        \n",
    "    te_num_nonzero, te_repeat_score, te_num_repeats = [], [], []\n",
    "    for comment in processed_te_comments:\n",
    "        vec = comment.strip().split() \n",
    "        te_num_nonzero.append( len(set(vec)))\n",
    "        te_num_repeats.append( len(vec))\n",
    "        te_rep_score = ( len( set( vec)) + 1)/(len(vec) + 1) - 1\n",
    "        te_repeat_score.append( te_rep_score) \n",
    "\n",
    "    # Create Feature DataFrame\n",
    "    test_feat['repeat_score'] = te_repeat_score\n",
    "    train_feat['repeat_score'] = tr_repeat_score \n",
    "\n",
    "    \n",
    "if DO_NUM_NONZERO:\n",
    "    train_feat['num_nonzero'] = tr_num_nonzero\n",
    "    test_feat['num_nonzero'] = te_num_nonzero\n",
    "    \n",
    "if DO_NUM_REPEATS: \n",
    "    test_feat['num_repeats'] = te_num_repeats\n",
    "    train_feat['num_repeats'] = tr_num_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CURSE WORD COUNTER \n",
    "if DO_CURSEWORD_COUNT: \n",
    "    curseword_file = 'wordlists/cursewords.csv'\n",
    "    curseword_list = []\n",
    "    with open( curseword_file , 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        curseword_list = list(reader)\n",
    "\n",
    "    curseword_list = [l[0] for l in curseword_list]\n",
    "\n",
    "    # regexp preprocessing \n",
    "    # (Different than for repeat_score, we're keeping digits because some cursewords contain digits, \n",
    "    # e.g., assh0le )\n",
    "    RE_PLUS_URL_PREPROCESS = re.compile(r\"\"\"\\W+ | [-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\"\"\", re.VERBOSE)\n",
    "    train_comments = list( tr.comment_text )\n",
    "    test_comments = list( te.comment_text )\n",
    "    processed_tr_comments = [ re.sub(RE_PLUS_URL_PREPROCESS, ' ', comment).lower() \\\n",
    "                             for comment in train_comments]\n",
    "    processed_te_comments = [ re.sub(RE_PLUS_URL_PREPROCESS, ' ', comment).lower() \\\n",
    "                             for comment in test_comments] \n",
    "\n",
    "    curse_overlap = set(curseword_list).intersection( tr_vocab2 )\n",
    "    indices = [ tr_vocab2.index( word ) for word in curse_overlap]\n",
    "\n",
    "    train_feat['curseword_count'] = train_bag_of_words[:, indices].sum(1)\n",
    "    test_feat['curseword_count'] = test_bag_of_words[:, indices].sum(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
